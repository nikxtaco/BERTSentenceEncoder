# BERTSentenceEncoder

Includes a custom implementation of BERT (Bidirectional Encoder Representations from Transformers), finetuned using the NLI (Natural Language Inference) dataset constructing a classification based model and a contrastive approach based model. The three BERT models are evaluated using the STS (Semantic Textual Similarity) benchmark, and the Pearson & Spearman scores are compared.

[Open in colab](https://colab.research.google.com/github/nikxtaco/BERTSentenceEncoder/blob/master/bert_sentence_encoder.ipynb)
